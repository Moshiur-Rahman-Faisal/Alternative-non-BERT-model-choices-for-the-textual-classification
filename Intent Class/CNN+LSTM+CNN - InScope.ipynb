{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-6hmel6sKNSP"
   },
   "outputs": [],
   "source": [
    "def reproduceResult():\n",
    "  seed_value= 0\n",
    "\n",
    "  \n",
    "  with tf.device(\"/cpu:0\"):\n",
    "    ...\n",
    "\n",
    "\n",
    "  os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "  np.random.seed(0)\n",
    "  rn.seed(0)\n",
    "\n",
    "\n",
    "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, \n",
    "                                          inter_op_parallelism_threads=1)\n",
    "\n",
    "\n",
    "  tf.compat.v1.set_random_seed(seed_value)\n",
    "  sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "  tf.compat.v1.keras.backend.clear_session()\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vURLkAC5_Jp0",
    "outputId": "4e1902aa-5a6c-48c4-ade4-dee710249417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_21284/4179402048.py:20: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_21284/628588012.py:43: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "  \n",
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "from tensorflow import keras\n",
    "\n",
    "reproduceResult()\n",
    "# %tensorflow_version 2.x\n",
    "# import tensorflow as tf\n",
    "# tf.test.gpu_device_name()\n",
    "# from scipy import integrate\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from tensorflow import keras\n",
    "import tempfile\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "# import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "from keras_lr_finder import LRFinder\n",
    "from clr.clr_callback import CyclicLR\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "\n",
    "\n",
    "import keras_tuner\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set: 15000\n",
      "size of testing set: 4500\n",
      "size of validation set: 3000\n",
      "translate          150\n",
      "order_status       150\n",
      "goodbye            150\n",
      "account_blocked    150\n",
      "what_song          150\n",
      "                  ... \n",
      "reminder           150\n",
      "change_speed       150\n",
      "tire_pressure      150\n",
      "no                 150\n",
      "card_declined      150\n",
      "Name: Intent, Length: 150, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what expression would i use to say i love you ...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can you tell me how to say 'i do not speak muc...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is the equivalent of, 'life is good' in f...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tell me how to say, 'it is a beautiful morning...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if i were mongolian, how would i say that i am...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>how do i say 'hotel' in finnish</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i need you to translate the sentence, 'we will...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>please tell me how to ask for a taxi in french</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>can you tell me how i would say, 'more bread p...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what is the correct way to say 'i am a visitor...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Utterance     Intent\n",
       "0  what expression would i use to say i love you ...  translate\n",
       "1  can you tell me how to say 'i do not speak muc...  translate\n",
       "2  what is the equivalent of, 'life is good' in f...  translate\n",
       "3  tell me how to say, 'it is a beautiful morning...  translate\n",
       "4  if i were mongolian, how would i say that i am...  translate\n",
       "5                    how do i say 'hotel' in finnish  translate\n",
       "6  i need you to translate the sentence, 'we will...  translate\n",
       "7     please tell me how to ask for a taxi in french  translate\n",
       "8  can you tell me how i would say, 'more bread p...  translate\n",
       "9  what is the correct way to say 'i am a visitor...  translate"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('IR_Train.csv',encoding= 'unicode_escape') \n",
    "data_test = pd.read_csv('IR_Test.csv',encoding= 'unicode_escape')\n",
    "data_val= pd.read_csv('IR_Val.csv',encoding= 'unicode_escape')\n",
    "\n",
    "X_train = data_train.Utterance.tolist()\n",
    "X_test = data_test.Utterance.tolist()\n",
    "X_val = data_train.Utterance.tolist()\n",
    "\n",
    "y_train = data_train.Intent.tolist()\n",
    "y_test = data_test.Intent.tolist()\n",
    "Y_val = data_test.Intent.tolist()\n",
    "\n",
    "\n",
    "\n",
    "data = data_train.append(data_test, ignore_index=True)\n",
    "temp= data.append(data_val, ignore_index=True)\n",
    "\n",
    "\n",
    "print('size of training set: %s' % (len(data_train['Utterance'])))\n",
    "print('size of testing set: %s' % (len(data_test['Utterance'])))\n",
    "print('size of validation set: %s' % (len(data_val['Utterance'])))\n",
    "print(temp.Intent.value_counts())\n",
    "\n",
    "temp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intent</th>\n",
       "      <th>Intent_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Intent  Intent_label\n",
       "0   translate           131\n",
       "1   translate           131\n",
       "2   translate           131\n",
       "3   translate           131\n",
       "4   translate           131\n",
       "5   translate           131\n",
       "6   translate           131\n",
       "7   translate           131\n",
       "8   translate           131\n",
       "9   translate           131\n",
       "10  translate           131"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "temp[\"Intent_label\"] = lb_make.fit_transform(temp[\"Intent\"])\n",
    "temp[[\"Intent\", \"Intent_label\"]].head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 5822\n",
      "[[  0   0   0 ... 167 369  19]\n",
      " [  0   0   0 ...  16 799 231]\n",
      " [  0   0   0 ...   1  73  18]\n",
      " ...\n",
      " [  0   0   0 ...  89 646  35]\n",
      " [  0   0   0 ...   1 305 113]\n",
      " [  0   0   0 ...   3  71  26]]\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(temp, test_size=0.2, stratify = temp['Intent_label'], random_state = 42)\n",
    "num_classes = 150\n",
    "embed_num_dims = 300\n",
    "max_seq_len = 50\n",
    "\n",
    "x_train = train['Utterance']\n",
    "x_test = test['Utterance']\n",
    "\n",
    "y_traibn = train['Utterance']\n",
    "y_test = test['Utterance']\n",
    "\n",
    "texts_train = x_train\n",
    "texts_test = x_test\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train['Utterance'])\n",
    "\n",
    "sequence_train = tokenizer.texts_to_sequences(texts_train)\n",
    "sequence_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "\n",
    "vocab_size = len(index_of_words) + 1\n",
    "\n",
    "print('Number of unique words: {}'.format(len(index_of_words)))\n",
    "\n",
    "X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len, padding='pre' )\n",
    "X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len,  padding='pre')\n",
    "\n",
    "print(X_train_pad)\n",
    "\n",
    "\n",
    "\n",
    "y_train = train['Intent_label']\n",
    "y_test = test['Intent_label']\n",
    "\n",
    "\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5823, 300)\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    with open('cc.en.300.vec',encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
    "print(embedd_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGgsd5mMZPKn"
   },
   "source": [
    "# Random Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IvOZoK8YGDI",
    "outputId": "61a9c6fc-15e1-4fbe-eed4-50296cc2b821",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 11m 16s]\n",
      "val_accuracy: 0.9268888831138611\n",
      "\n",
      "Best val_accuracy So Far: 0.9328888654708862\n",
      "Total elapsed time: 10h 02m 21s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in 1649911142\\untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 80\n",
      "cnn_1_unit: 64\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 160\n",
      "lstm_dropout: 0.1\n",
      "cnn_2_unit: 224\n",
      "cnn_2_dropout: 0.30000000000000004\n",
      "Score: 0.9328888654708862\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 48\n",
      "cnn_1_unit: 96\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 128\n",
      "lstm_dropout: 0.30000000000000004\n",
      "cnn_2_unit: 128\n",
      "cnn_2_dropout: 0.4\n",
      "Score: 0.9313333630561829\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 96\n",
      "cnn_1_unit: 48\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 160\n",
      "lstm_dropout: 0.1\n",
      "cnn_2_unit: 160\n",
      "cnn_2_dropout: 0.30000000000000004\n",
      "Score: 0.9311110973358154\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 112\n",
      "cnn_1_unit: 96\n",
      "cnn_1_dropout: 0.30000000000000004\n",
      "lstm_unit: 96\n",
      "lstm_dropout: 0.4\n",
      "cnn_2_unit: 192\n",
      "cnn_2_dropout: 0.30000000000000004\n",
      "Score: 0.9308888912200928\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 112\n",
      "cnn_1_unit: 80\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 96\n",
      "lstm_dropout: 0.30000000000000004\n",
      "cnn_2_unit: 128\n",
      "cnn_2_dropout: 0.1\n",
      "Score: 0.9308888912200928\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 80\n",
      "cnn_1_unit: 96\n",
      "cnn_1_dropout: 0.30000000000000004\n",
      "lstm_unit: 224\n",
      "lstm_dropout: 0.2\n",
      "cnn_2_unit: 160\n",
      "cnn_2_dropout: 0.30000000000000004\n",
      "Score: 0.9308888912200928\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 96\n",
      "cnn_1_unit: 80\n",
      "cnn_1_dropout: 0.1\n",
      "lstm_unit: 192\n",
      "lstm_dropout: 0.4\n",
      "cnn_2_unit: 160\n",
      "cnn_2_dropout: 0.30000000000000004\n",
      "Score: 0.93022221326828\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 96\n",
      "cnn_1_unit: 32\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 160\n",
      "lstm_dropout: 0.2\n",
      "cnn_2_unit: 160\n",
      "cnn_2_dropout: 0.4\n",
      "Score: 0.93022221326828\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 80\n",
      "cnn_1_unit: 80\n",
      "cnn_1_dropout: 0.30000000000000004\n",
      "lstm_unit: 128\n",
      "lstm_dropout: 0.1\n",
      "cnn_2_unit: 160\n",
      "cnn_2_dropout: 0.30000000000000004\n",
      "Score: 0.9300000071525574\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 80\n",
      "cnn_1_unit: 80\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 192\n",
      "lstm_dropout: 0.1\n",
      "cnn_2_unit: 64\n",
      "cnn_2_dropout: 0.2\n",
      "Score: 0.9295555353164673\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "import time\n",
    "LOG_DIR = f\"{int(time.time())}\"\n",
    "seed_value= 0\n",
    "\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "  \n",
    "  reproduceResult()\n",
    "\n",
    "  print('Ya it comes here')\n",
    "  unit_attention = hp.Int(\"attention_unit\",min_value =32, max_value = 128, step = 16)\n",
    "  fake_val = hp.Int(\"cnn_1_unit\",min_value =16, max_value = 96, step = 16)\n",
    "  cnn_1_unit = hp.Int(\"cnn_1_unit\",min_value =16, max_value = 96, step = 16)\n",
    "  cnn_1_dropout = hp.Float(\"cnn_1_dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
    "\n",
    "  lstm_unit = hp.Int(\"lstm_unit\",min_value =64, max_value = 256, step = 32)\n",
    "  lstm_dropout = hp.Float(\"lstm_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
    "  cnn_2_unit = hp.Int(\"cnn_2_unit\",min_value =64, max_value = 256, step = 32)\n",
    "  cnn_2_dropout = hp.Float(\"cnn_2_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
    "\n",
    "\n",
    "\n",
    "  seq_input = keras.layers.Input(shape=(max_seq_len,))\n",
    "\n",
    "  embedded = keras.layers.Embedding(vocab_size,\n",
    "                          embed_num_dims,\n",
    "                          input_length = max_seq_len,\n",
    "                          weights = [embedd_matrix])(seq_input)\n",
    "\n",
    "  cnn = keras.layers.Conv1D(cnn_1_unit,3,kernel_regularizer=regularizers.l2(1e-4),\n",
    "                            bias_regularizer=regularizers.l2(1e-2),\n",
    "                            activity_regularizer=regularizers.l2(1e-4))(embedded)\n",
    "  cnn = keras.layers.Activation(activation='relu')(cnn)\n",
    "  cnn = keras.layers.BatchNormalization()(cnn)\n",
    "  cnn = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn)\n",
    "\n",
    "\n",
    "  lstm =keras.layers.LSTM(lstm_unit, recurrent_regularizer=regularizers.l2(1e-4),\n",
    "                                                      return_sequences=True,kernel_regularizer=regularizers.l2(1e-4),\n",
    "                                                      bias_regularizer=regularizers.l2(1e-2),\n",
    "                                                      activity_regularizer=regularizers.l2(1e-4),input_shape =(48,))(cnn)\n",
    "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
    "  lstm = keras.layers.BatchNormalization()(lstm)\n",
    "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
    "  \n",
    "  \n",
    "\n",
    "  cnn_2 = keras.layers.Conv1D(cnn_2_unit,3,kernel_regularizer=regularizers.l2(1e-4),\n",
    "                            bias_regularizer=regularizers.l2(1e-2),\n",
    "                            activity_regularizer=regularizers.l2(1e-4))(lstm)\n",
    "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
    "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
    "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
    "\n",
    "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_2)\n",
    "  output = keras.layers.Dense(num_classes, activation='softmax')(max_pooling)\n",
    "\n",
    "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
    "  model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                              patience=4,\n",
    "                              restore_best_weights=True,\n",
    "                              verbose=0, mode='max')\n",
    "\n",
    "\n",
    "clr_step_size = int((len(X_train_pad)/64))\n",
    "base_lr = 1e-3\n",
    "max_lr = 6e-3\n",
    "mode = 'exp_range'\n",
    "\n",
    "\n",
    "clr = CyclicLR(base_lr = base_lr, max_lr = max_lr, step_size = clr_step_size, mode = mode)\n",
    "\n",
    "\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
    "    max_trials = 50,\n",
    "    executions_per_trial = 1,\n",
    "    directory = LOG_DIR\n",
    "    )\n",
    "  \n",
    "tuner.search(x=X_train_pad,y = y_train,epochs = 20, batch_size = 128,callbacks = [stop,clr], \n",
    "             validation_data = (X_test_pad,y_test))\n",
    "\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "100_percent_test_cnn_(A)lstm_cnn_best_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
