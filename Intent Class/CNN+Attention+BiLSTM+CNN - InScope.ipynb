{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-6hmel6sKNSP"
   },
   "outputs": [],
   "source": [
    "def reproduceResult():\n",
    "  seed_value= 0\n",
    "\n",
    "  \n",
    "  with tf.device(\"/cpu:0\"):\n",
    "    ...\n",
    "\n",
    "\n",
    "  os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "  np.random.seed(0)\n",
    "  rn.seed(0)\n",
    "\n",
    "\n",
    "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, \n",
    "                                          inter_op_parallelism_threads=1)\n",
    "\n",
    "\n",
    "  tf.compat.v1.set_random_seed(seed_value)\n",
    "  sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "  tf.compat.v1.keras.backend.clear_session()\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vURLkAC5_Jp0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_12540/4179402048.py:20: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_12540/628588012.py:43: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "  \n",
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "from tensorflow import keras\n",
    "\n",
    "reproduceResult()\n",
    "# %tensorflow_version 2.x\n",
    "# import tensorflow as tf\n",
    "# tf.test.gpu_device_name()\n",
    "# from scipy import integrate\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from tensorflow import keras\n",
    "import tempfile\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "# import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "from keras_lr_finder import LRFinder\n",
    "from clr.clr_callback import CyclicLR\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "\n",
    "\n",
    "import keras_tuner\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set: 15000\n",
      "size of testing set: 4500\n",
      "size of validation set: 3000\n",
      "translate          150\n",
      "order_status       150\n",
      "goodbye            150\n",
      "account_blocked    150\n",
      "what_song          150\n",
      "                  ... \n",
      "reminder           150\n",
      "change_speed       150\n",
      "tire_pressure      150\n",
      "no                 150\n",
      "card_declined      150\n",
      "Name: Intent, Length: 150, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what expression would i use to say i love you ...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can you tell me how to say 'i do not speak muc...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is the equivalent of, 'life is good' in f...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tell me how to say, 'it is a beautiful morning...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if i were mongolian, how would i say that i am...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>how do i say 'hotel' in finnish</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i need you to translate the sentence, 'we will...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>please tell me how to ask for a taxi in french</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>can you tell me how i would say, 'more bread p...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what is the correct way to say 'i am a visitor...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Utterance     Intent\n",
       "0  what expression would i use to say i love you ...  translate\n",
       "1  can you tell me how to say 'i do not speak muc...  translate\n",
       "2  what is the equivalent of, 'life is good' in f...  translate\n",
       "3  tell me how to say, 'it is a beautiful morning...  translate\n",
       "4  if i were mongolian, how would i say that i am...  translate\n",
       "5                    how do i say 'hotel' in finnish  translate\n",
       "6  i need you to translate the sentence, 'we will...  translate\n",
       "7     please tell me how to ask for a taxi in french  translate\n",
       "8  can you tell me how i would say, 'more bread p...  translate\n",
       "9  what is the correct way to say 'i am a visitor...  translate"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('IR_Train.csv',encoding= 'unicode_escape') \n",
    "data_test = pd.read_csv('IR_Test.csv',encoding= 'unicode_escape')\n",
    "data_val= pd.read_csv('IR_Val.csv',encoding= 'unicode_escape')\n",
    "\n",
    "X_train = data_train.Utterance.tolist()\n",
    "X_test = data_test.Utterance.tolist()\n",
    "X_val = data_train.Utterance.tolist()\n",
    "\n",
    "y_train = data_train.Intent.tolist()\n",
    "y_test = data_test.Intent.tolist()\n",
    "Y_val = data_test.Intent.tolist()\n",
    "\n",
    "\n",
    "\n",
    "data = data_train.append(data_test, ignore_index=True)\n",
    "temp= data.append(data_val, ignore_index=True)\n",
    "\n",
    "\n",
    "print('size of training set: %s' % (len(data_train['Utterance'])))\n",
    "print('size of testing set: %s' % (len(data_test['Utterance'])))\n",
    "print('size of validation set: %s' % (len(data_val['Utterance'])))\n",
    "print(temp.Intent.value_counts())\n",
    "\n",
    "temp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intent</th>\n",
       "      <th>Intent_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>translate</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Intent  Intent_label\n",
       "0   translate           131\n",
       "1   translate           131\n",
       "2   translate           131\n",
       "3   translate           131\n",
       "4   translate           131\n",
       "5   translate           131\n",
       "6   translate           131\n",
       "7   translate           131\n",
       "8   translate           131\n",
       "9   translate           131\n",
       "10  translate           131"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "temp[\"Intent_label\"] = lb_make.fit_transform(temp[\"Intent\"])\n",
    "temp[[\"Intent\", \"Intent_label\"]].head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFgD7Seo_Xlq",
    "outputId": "9be2fe5a-b7b5-4488-9cd4-8a48ce6123e8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 5822\n",
      "[[  0   0   0 ... 167 369  19]\n",
      " [  0   0   0 ...  16 799 231]\n",
      " [  0   0   0 ...   1  73  18]\n",
      " ...\n",
      " [  0   0   0 ...  89 646  35]\n",
      " [  0   0   0 ...   1 305 113]\n",
      " [  0   0   0 ...   3  71  26]]\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(temp, test_size=0.2, stratify = temp['Intent_label'], random_state = 42)\n",
    "num_classes = 150\n",
    "embed_num_dims = 300\n",
    "max_seq_len = 50\n",
    "\n",
    "x_train = train['Utterance']\n",
    "x_test = test['Utterance']\n",
    "\n",
    "y_train = train['Utterance']\n",
    "y_test = test['Utterance']\n",
    "\n",
    "texts_train = x_train\n",
    "texts_test = x_test\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train['Utterance'])\n",
    "\n",
    "sequence_train = tokenizer.texts_to_sequences(texts_train)\n",
    "sequence_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "\n",
    "vocab_size = len(index_of_words) + 1\n",
    "\n",
    "print('Number of unique words: {}'.format(len(index_of_words)))\n",
    "\n",
    "X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len, padding='pre' )\n",
    "X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len,  padding='pre')\n",
    "\n",
    "print(X_train_pad)\n",
    "\n",
    "\n",
    "\n",
    "y_train = train['Intent_label']\n",
    "y_test = test['Intent_label']\n",
    "\n",
    "\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5823, 300)\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    with open('cc.en.300.vec',encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
    "print(embedd_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGgsd5mMZPKn"
   },
   "source": [
    "\n",
    "# Random Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IvOZoK8YGDI",
    "outputId": "fe861031-a89e-45d3-8f7a-42f1e7b6b256",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40 Complete [00h 17m 18s]\n",
      "val_accuracy: 0.8640000224113464\n",
      "\n",
      "Best val_accuracy So Far: 0.9200000166893005\n",
      "Total elapsed time: 14h 05m 57s\n",
      "\n",
      "Search: Running Trial #41\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "attention_unit    |80                |48                \n",
      "cnn_1_unit        |96                |48                \n",
      "cnn_1_dropout     |0.3               |0.2               \n",
      "lstm_unit         |224               |160               \n",
      "lstm_dropout      |0.2               |0.1               \n",
      "cnn_2_unit        |160               |224               \n",
      "cnn_2_dropout     |0.3               |0.5               \n",
      "\n",
      "Ya it comes here\n",
      "Epoch 1/20\n",
      "141/141 [==============================] - 148s 1s/step - loss: 10.8019 - accuracy: 0.0626 - val_loss: 7.4800 - val_accuracy: 0.0180\n",
      "Epoch 2/20\n",
      "141/141 [==============================] - 139s 983ms/step - loss: 5.7749 - accuracy: 0.2362 - val_loss: 4.9234 - val_accuracy: 0.1962\n",
      "Epoch 3/20\n",
      "141/141 [==============================] - 139s 986ms/step - loss: 4.6370 - accuracy: 0.4246 - val_loss: 4.2837 - val_accuracy: 0.5271\n",
      "Epoch 4/20\n",
      "141/141 [==============================] - 139s 984ms/step - loss: 2.4640 - accuracy: 0.6550 - val_loss: 2.2677 - val_accuracy: 0.7382\n",
      "Epoch 5/20\n",
      "141/141 [==============================] - 139s 986ms/step - loss: 1.8070 - accuracy: 0.7299 - val_loss: 1.9622 - val_accuracy: 0.7340\n",
      "Epoch 6/20\n",
      "141/141 [==============================] - 141s 1s/step - loss: 1.8378 - accuracy: 0.7112 - val_loss: 5.7784 - val_accuracy: 0.4689\n",
      "Epoch 7/20\n",
      "141/141 [==============================] - 140s 990ms/step - loss: 1.6562 - accuracy: 0.7417 - val_loss: 1.4558 - val_accuracy: 0.7940\n",
      "Epoch 8/20\n",
      "141/141 [==============================] - 142s 1s/step - loss: 0.9912 - accuracy: 0.8479 - val_loss: 1.1732 - val_accuracy: 0.8262\n",
      "Epoch 9/20\n",
      "141/141 [==============================] - 142s 1s/step - loss: 0.8304 - accuracy: 0.8795 - val_loss: 1.0440 - val_accuracy: 0.8424\n",
      "Epoch 10/20\n",
      "141/141 [==============================] - 138s 980ms/step - loss: 0.9055 - accuracy: 0.8537 - val_loss: 1.1688 - val_accuracy: 0.7927\n",
      "Epoch 11/20\n",
      "141/141 [==============================] - 138s 980ms/step - loss: 0.9964 - accuracy: 0.8404 - val_loss: 1.7798 - val_accuracy: 0.7582\n",
      "Epoch 12/20\n",
      "141/141 [==============================] - 138s 979ms/step - loss: 0.8192 - accuracy: 0.8901 - val_loss: 0.9142 - val_accuracy: 0.8738\n",
      "Epoch 13/20\n",
      "141/141 [==============================] - 138s 976ms/step - loss: 0.6810 - accuracy: 0.9143 - val_loss: 1.1202 - val_accuracy: 0.8044\n",
      "Epoch 14/20\n",
      "141/141 [==============================] - 137s 975ms/step - loss: 0.7821 - accuracy: 0.8811 - val_loss: 3.8710 - val_accuracy: 0.4907\n",
      "Epoch 15/20\n",
      "141/141 [==============================] - 138s 979ms/step - loss: 1.0209 - accuracy: 0.8448 - val_loss: 2.6843 - val_accuracy: 0.6056\n",
      "Epoch 16/20\n",
      "141/141 [==============================] - 138s 978ms/step - loss: 0.7338 - accuracy: 0.9141 - val_loss: 0.8941 - val_accuracy: 0.8749\n",
      "Epoch 17/20\n",
      "141/141 [==============================] - 140s 993ms/step - loss: 0.6430 - accuracy: 0.9297 - val_loss: 0.9229 - val_accuracy: 0.8642\n",
      "Epoch 18/20\n",
      "141/141 [==============================] - 140s 991ms/step - loss: 0.8693 - accuracy: 0.8762 - val_loss: 3.1792 - val_accuracy: 0.7038\n",
      "Epoch 19/20\n",
      "141/141 [==============================] - 139s 985ms/step - loss: 1.0712 - accuracy: 0.8544 - val_loss: 1.1657 - val_accuracy: 0.8302\n",
      "Epoch 20/20\n",
      "132/141 [===========================>..] - ETA: 8s - loss: 0.7929 - accuracy: 0.9165"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "import time\n",
    "LOG_DIR = f\"{int(time.time())}\"\n",
    "seed_value= 0\n",
    "\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "  \n",
    "  reproduceResult()\n",
    "\n",
    "  print('Ya it comes here')\n",
    "  unit_attention = hp.Int(\"attention_unit\",min_value =32, max_value = 128, step = 16)\n",
    "  fake_val = hp.Int(\"cnn_1_unit\",min_value =16, max_value = 96, step = 16)\n",
    "  cnn_1_unit = hp.Int(\"cnn_1_unit\",min_value =16, max_value = 96, step = 16)\n",
    "  cnn_1_dropout = hp.Float(\"cnn_1_dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
    "\n",
    "  lstm_unit = hp.Int(\"lstm_unit\",min_value =64, max_value = 256, step = 32)\n",
    "  lstm_dropout = hp.Float(\"lstm_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
    "  cnn_2_unit = hp.Int(\"cnn_2_unit\",min_value =64, max_value = 256, step = 32)\n",
    "  cnn_2_dropout = hp.Float(\"cnn_2_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
    "\n",
    "\n",
    "\n",
    "  seq_input = keras.layers.Input(shape=(max_seq_len,))\n",
    "\n",
    "  embedded = keras.layers.Embedding(vocab_size,\n",
    "                          embed_num_dims,\n",
    "                          input_length = max_seq_len,\n",
    "                          weights = [embedd_matrix])(seq_input)\n",
    "\n",
    "  cnn = keras.layers.Conv1D(cnn_1_unit,3,kernel_regularizer=regularizers.l2(1e-4),\n",
    "                            bias_regularizer=regularizers.l2(1e-2),\n",
    "                            activity_regularizer=regularizers.l2(1e-4))(embedded)\n",
    "  cnn = keras.layers.Activation(activation='relu')(cnn)\n",
    "  cnn = keras.layers.BatchNormalization()(cnn)\n",
    "  cnn = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn)\n",
    "\n",
    "  attention_vec = keras.layers.TimeDistributed(keras.layers.Dense(unit_attention))(cnn)\n",
    "  attention_vec = keras.layers.Reshape((48,unit_attention))(attention_vec)\n",
    "  attention_vec = keras.layers.Activation('relu', name = 'cnn_attention_vec')(attention_vec)\n",
    "  attention_output = keras.layers.Dot(axes = 1)([cnn, attention_vec])\n",
    "\n",
    "\n",
    "  lstm = keras.layers.Bidirectional(keras.layers.LSTM(lstm_unit, recurrent_regularizer=regularizers.l2(1e-4),\n",
    "                                                      return_sequences=True,kernel_regularizer=regularizers.l2(1e-4),\n",
    "                                                      bias_regularizer=regularizers.l2(1e-2),\n",
    "                                                      activity_regularizer=regularizers.l2(1e-4),input_shape =(48,)))(attention_output)\n",
    "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
    "  lstm = keras.layers.BatchNormalization()(lstm)\n",
    "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
    "  \n",
    "  \n",
    "\n",
    "  cnn_2 = keras.layers.Conv1D(cnn_2_unit,3,kernel_regularizer=regularizers.l2(1e-4),\n",
    "                            bias_regularizer=regularizers.l2(1e-2),\n",
    "                            activity_regularizer=regularizers.l2(1e-4))(lstm)\n",
    "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
    "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
    "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
    "\n",
    "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_2)\n",
    "  output = keras.layers.Dense(num_classes, activation='softmax')(max_pooling)\n",
    "\n",
    "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
    "  model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                              patience=4,\n",
    "                              restore_best_weights=True,\n",
    "                              verbose=0, mode='max')\n",
    "\n",
    "\n",
    "clr_step_size = int((len(X_train_pad)/64))\n",
    "base_lr = 1e-3\n",
    "max_lr = 6e-3\n",
    "mode = 'exp_range'\n",
    "\n",
    "\n",
    "clr = CyclicLR(base_lr = base_lr, max_lr = max_lr, step_size = clr_step_size, mode = mode)\n",
    "\n",
    "\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
    "    max_trials = 50,\n",
    "    executions_per_trial = 1,\n",
    "    directory = LOG_DIR\n",
    "    )\n",
    "  \n",
    "tuner.search(x=X_train_pad,y = y_train,epochs = 20, batch_size = 128,callbacks = [stop,clr], \n",
    "             validation_data = (X_test_pad,y_test))\n",
    "\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "100_percent_test_BiLSTM_best_model_git.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
