{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-6hmel6sKNSP"
   },
   "outputs": [],
   "source": [
    "def reproduceResult():\n",
    "  seed_value= 0\n",
    "\n",
    "  \n",
    "  with tf.device(\"/cpu:0\"):\n",
    "    ...\n",
    "\n",
    "\n",
    "  os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "  np.random.seed(0)\n",
    "  rn.seed(0)\n",
    "\n",
    "\n",
    "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, \n",
    "                                          inter_op_parallelism_threads=1)\n",
    "\n",
    "\n",
    "  tf.compat.v1.set_random_seed(seed_value)\n",
    "  sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "  tf.compat.v1.keras.backend.clear_session()\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vURLkAC5_Jp0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_7680/4179402048.py:20: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_7680/628588012.py:43: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "  \n",
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "from tensorflow import keras\n",
    "\n",
    "reproduceResult()\n",
    "# %tensorflow_version 2.x\n",
    "# import tensorflow as tf\n",
    "# tf.test.gpu_device_name()\n",
    "# from scipy import integrate\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from tensorflow import keras\n",
    "import tempfile\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "# import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "from keras_lr_finder import LRFinder\n",
    "from clr.clr_callback import CyclicLR\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "\n",
    "\n",
    "import keras_tuner\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moshi\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\moshi\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('wnli.csv',index_col=False, encoding='iso-8859-1', \n",
    "                                warn_bad_lines=True, error_bad_lines=False)\n",
    "\n",
    "df2 = pd.read_csv('wnli.csv',index_col=False, encoding='iso-8859-1', \n",
    "                                warn_bad_lines=True, error_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_7680/3345057914.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1.label[df1.label == 1]=\"TRUE\"\n",
      "C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_7680/3345057914.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1.label[df1.label == 0]=\"FALSE\"\n",
      "C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_7680/3345057914.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2.label[df2.label == 1]=\"TRUE\"\n",
      "C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_7680/3345057914.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2.label[df2.label == 0]=\"FALSE\"\n"
     ]
    }
   ],
   "source": [
    "df1.label[df1.label == 1]=\"TRUE\"\n",
    "df1.label[df1.label == 0]=\"FALSE\"\n",
    "\n",
    "df2.label[df2.label == 1]=\"TRUE\"\n",
    "df2.label[df2.label == 0]=\"FALSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FALSE    323\n",
       "TRUE     312\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FALSE    323\n",
       "TRUE     312\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking acceptable and deleting unacceptable\n",
    "df3 = df1[~df1.label.str.contains('TRUE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       index                                          sentence1  \\\n",
       "3         3  Steve follows Fred's example in everything. He...   \n",
       "4         4  When Tatyana reached the cabin, her mother was...   \n",
       "5         5  George got free tickets to the play, but he ga...   \n",
       "6         6  John was jogging through the park when he saw ...   \n",
       "9         9  At the Loebner competition the judges couldn't...   \n",
       "..      ...                                                ...   \n",
       "624     624  I used an old rag to clean the knife, and then...   \n",
       "625     625  Since it was raining, I carried the newspaper ...   \n",
       "628     628  By rolling over in her upper berth, Tatyana co...   \n",
       "630     630  When Tommy dropped his ice cream, Timmy giggle...   \n",
       "632     632   Tom said \"Check\" to Ralph as he took his bishop.   \n",
       "\n",
       "                                             sentence2  label  \n",
       "3                         Steve influences him hugely.  FALSE  \n",
       "4    mother was careful not to disturb her, undress...  FALSE  \n",
       "5             George was particularly eager to see it.  FALSE  \n",
       "6                            John was very impressive.  FALSE  \n",
       "9                         The judges were so advanced.  FALSE  \n",
       "..                                                 ...    ...  \n",
       "624                      I put the knife in the trash.  FALSE  \n",
       "625  I carried the newspaper in my backpack to keep...  FALSE  \n",
       "628                  Tatyana doubted if Tatyana slept.  FALSE  \n",
       "630              Father gave Timmy a sympathetic look.  FALSE  \n",
       "632  Tom said \"Check\" to Ralph as he took Tom's bis...  FALSE  \n",
       "\n",
       "[323 rows x 4 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomly picking 250 rows\n",
    "df3 = df3.sample(n=250, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FALSE    250\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking unacceptable and deleting unacceptable\n",
    "df4 = df2[~df2.label.str.contains(\"FALSE\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly picking 250 rows\n",
    "df4 = df4.sample(n=250, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRUE    250\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concating acceptable and unacceptable\n",
    "frames = [df3, df4]\n",
    "\n",
    "temp = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FALSE    250\n",
       "TRUE     250\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFgD7Seo_Xlq",
    "outputId": "9be2fe5a-b7b5-4488-9cd4-8a48ce6123e8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 1226\n",
      "[[  0   0   0 ...  14 107  94]\n",
      " [ 18  32  70 ...  29   3  96]\n",
      " [  0   0   0 ... 289 471 226]\n",
      " ...\n",
      " [  0   0   0 ...  36 459 138]\n",
      " [  0   0   0 ...  15 246  46]\n",
      " [  0   0   0 ... 193   8 643]]\n",
      "(1227, 300)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(temp, test_size=0.2, stratify = temp['label'], random_state = 42)\n",
    "num_classes = 2\n",
    "embed_num_dims = 300\n",
    "max_seq_len = 50\n",
    "\n",
    "x_train = train['sentence1'] + train['sentence2']\n",
    "x_test = test['sentence1'] + test['sentence2']\n",
    "\n",
    "y_train = train['label']\n",
    "y_test = test['label']\n",
    "\n",
    "texts_train = x_train\n",
    "texts_test = x_test\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train['sentence1'] + train['sentence2'])\n",
    "\n",
    "sequence_train = tokenizer.texts_to_sequences(texts_train)\n",
    "sequence_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "\n",
    "vocab_size = len(index_of_words) + 1\n",
    "\n",
    "print('Number of unique words: {}'.format(len(index_of_words)))\n",
    "\n",
    "X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len, padding='pre' )\n",
    "X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len,  padding='pre')\n",
    "\n",
    "print(X_train_pad)\n",
    "\n",
    "\n",
    "encoding = {\n",
    "    'TRUE': 1,\n",
    "    'FALSE': 0\n",
    "}\n",
    "\n",
    "y_train = [encoding[x] for x in train['label']]\n",
    "y_test = [encoding[x] for x in test['label']]\n",
    "\n",
    "\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "def create_embedding_matrix(word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    with open('C:/Users/moshi/Python Code/Untitled Folder/cc.en.300.vec',encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
    "print(embedd_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGgsd5mMZPKn"
   },
   "source": [
    "# Random Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n",
    "\n",
    "def get_flops(model):\n",
    "    concrete = tf.function(lambda inputs: model(inputs))\n",
    "    concrete_func = concrete.get_concrete_function(\n",
    "        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n",
    "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.graph_util.import_graph_def(graph_def, name='')\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n",
    "        return flops.total_float_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IvOZoK8YGDI",
    "outputId": "fe861031-a89e-45d3-8f7a-42f1e7b6b256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40 Complete [00h 01m 42s]\n",
      "val_accuracy: 0.6200000047683716\n",
      "\n",
      "Best val_accuracy So Far: 0.6499999761581421\n",
      "Total elapsed time: 01h 45m 29s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in 1653762962\\untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 96\n",
      "cnn_1_unit: 64\n",
      "cnn_1_dropout: 0.1\n",
      "lstm_unit: 64\n",
      "lstm_dropout: 0.4\n",
      "cnn_2_unit: 128\n",
      "cnn_2_dropout: 0.4\n",
      "Score: 0.6499999761581421\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 80\n",
      "cnn_1_unit: 80\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 192\n",
      "lstm_dropout: 0.1\n",
      "cnn_2_unit: 64\n",
      "cnn_2_dropout: 0.2\n",
      "Score: 0.6399999856948853\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 112\n",
      "cnn_1_unit: 16\n",
      "cnn_1_dropout: 0.30000000000000004\n",
      "lstm_unit: 256\n",
      "lstm_dropout: 0.4\n",
      "cnn_2_unit: 256\n",
      "cnn_2_dropout: 0.1\n",
      "Score: 0.6299999952316284\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 80\n",
      "cnn_1_unit: 64\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 160\n",
      "lstm_dropout: 0.1\n",
      "cnn_2_unit: 224\n",
      "cnn_2_dropout: 0.30000000000000004\n",
      "Score: 0.6200000047683716\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 112\n",
      "cnn_1_unit: 80\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 96\n",
      "lstm_dropout: 0.30000000000000004\n",
      "cnn_2_unit: 128\n",
      "cnn_2_dropout: 0.1\n",
      "Score: 0.6200000047683716\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 64\n",
      "cnn_1_unit: 80\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 256\n",
      "lstm_dropout: 0.2\n",
      "cnn_2_unit: 160\n",
      "cnn_2_dropout: 0.2\n",
      "Score: 0.6100000143051147\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 128\n",
      "cnn_1_unit: 80\n",
      "cnn_1_dropout: 0.1\n",
      "lstm_unit: 256\n",
      "lstm_dropout: 0.4\n",
      "cnn_2_unit: 160\n",
      "cnn_2_dropout: 0.30000000000000004\n",
      "Score: 0.6100000143051147\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 128\n",
      "cnn_1_unit: 80\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 192\n",
      "lstm_dropout: 0.4\n",
      "cnn_2_unit: 160\n",
      "cnn_2_dropout: 0.2\n",
      "Score: 0.6100000143051147\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 48\n",
      "cnn_1_unit: 32\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 96\n",
      "lstm_dropout: 0.30000000000000004\n",
      "cnn_2_unit: 224\n",
      "cnn_2_dropout: 0.1\n",
      "Score: 0.6100000143051147\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 64\n",
      "cnn_1_unit: 64\n",
      "cnn_1_dropout: 0.1\n",
      "lstm_unit: 192\n",
      "lstm_dropout: 0.30000000000000004\n",
      "cnn_2_unit: 192\n",
      "cnn_2_dropout: 0.2\n",
      "Score: 0.6100000143051147\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "import time\n",
    "LOG_DIR = f\"{int(time.time())}\"\n",
    "seed_value= 0\n",
    "\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "  \n",
    "  reproduceResult()\n",
    "\n",
    "  print('Ya it comes here')\n",
    "  unit_attention = hp.Int(\"attention_unit\",min_value =32, max_value = 128, step = 16)\n",
    "  fake_val = hp.Int(\"cnn_1_unit\",min_value =16, max_value = 96, step = 16)\n",
    "  cnn_1_unit = hp.Int(\"cnn_1_unit\",min_value =16, max_value = 96, step = 16)\n",
    "  cnn_1_dropout = hp.Float(\"cnn_1_dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
    "\n",
    "  lstm_unit = hp.Int(\"lstm_unit\",min_value =64, max_value = 256, step = 32)\n",
    "  lstm_dropout = hp.Float(\"lstm_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
    "  cnn_2_unit = hp.Int(\"cnn_2_unit\",min_value =64, max_value = 256, step = 32)\n",
    "  cnn_2_dropout = hp.Float(\"cnn_2_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
    "\n",
    "\n",
    "\n",
    "  seq_input = keras.layers.Input(shape=(max_seq_len,))\n",
    "\n",
    "  embedded = keras.layers.Embedding(vocab_size,\n",
    "                          embed_num_dims,\n",
    "                          input_length = max_seq_len,\n",
    "                          weights = [embedd_matrix])(seq_input)\n",
    "\n",
    "  cnn = keras.layers.Conv1D(cnn_1_unit,3,kernel_regularizer=regularizers.l2(1e-4),\n",
    "                            bias_regularizer=regularizers.l2(1e-2),\n",
    "                            activity_regularizer=regularizers.l2(1e-4))(embedded)\n",
    "  cnn = keras.layers.Activation(activation='relu')(cnn)\n",
    "  cnn = keras.layers.BatchNormalization()(cnn)\n",
    "  cnn = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn)\n",
    "\n",
    "\n",
    "  lstm = keras.layers.Bidirectional(keras.layers.LSTM(lstm_unit, recurrent_regularizer=regularizers.l2(1e-4),\n",
    "                                                      return_sequences=True,kernel_regularizer=regularizers.l2(1e-4),\n",
    "                                                      bias_regularizer=regularizers.l2(1e-2),\n",
    "                                                      activity_regularizer=regularizers.l2(1e-4),input_shape =(48,)))(cnn)\n",
    "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
    "  lstm = keras.layers.BatchNormalization()(lstm)\n",
    "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
    "  \n",
    "  \n",
    "\n",
    "  cnn_2 = keras.layers.Conv1D(cnn_2_unit,3,kernel_regularizer=regularizers.l2(1e-4),\n",
    "                            bias_regularizer=regularizers.l2(1e-2),\n",
    "                            activity_regularizer=regularizers.l2(1e-4))(lstm)\n",
    "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
    "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
    "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
    "\n",
    "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_2)\n",
    "  output = keras.layers.Dense(num_classes, activation='softmax')(max_pooling)\n",
    "\n",
    "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
    "  model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "  model.summary()\n",
    "  \n",
    "  print(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True )\n",
    "    \n",
    "  print(\" \")\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "clr_step_size = int((len(X_train_pad)/64))\n",
    "base_lr = 1e-3\n",
    "max_lr = 6e-3\n",
    "mode = 'exp_range'\n",
    "\n",
    "\n",
    "clr = CyclicLR(base_lr = base_lr, max_lr = max_lr, step_size = clr_step_size, mode = mode)\n",
    "\n",
    "stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                              patience=5,\n",
    "                              restore_best_weights=True,\n",
    "                              verbose=0, mode='max')\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
    "    max_trials = 40,\n",
    "    executions_per_trial = 1,\n",
    "    directory = LOG_DIR\n",
    "    )\n",
    "  \n",
    "tuner.search(x=X_train_pad,y = y_train,epochs = 25, batch_size = 4, callbacks = [stop,clr], \n",
    "             validation_data = (X_test_pad,y_test))\n",
    "\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 7\n",
      "attention_unit (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 16, 'sampling': None}\n",
      "cnn_1_unit (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 96, 'step': 16, 'sampling': None}\n",
      "cnn_1_dropout (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.3, 'step': 0.1, 'sampling': None}\n",
      "lstm_unit (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 256, 'step': 32, 'sampling': None}\n",
      "lstm_dropout (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': None}\n",
      "cnn_2_unit (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 256, 'step': 32, 'sampling': None}\n",
      "cnn_2_dropout (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "100_percent_test_BiLSTM_best_model_git.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
